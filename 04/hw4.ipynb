{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNRegression():\n",
    "    # interno so posamezni primeri stolpci v matriki X, zunanje dosegljive funkcije pa obratno (kot zahtevano)\n",
    "    def __init__(self, units=[], lambda_=0):\n",
    "        self.units = units\n",
    "        self.lambda_ = lambda_\n",
    "        self.w = []\n",
    "\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_prime(self, a):\n",
    "        return a * (1 - a)\n",
    "    \n",
    "\n",
    "    # ti funkciji bosta drugacni za klasifikacijo\n",
    "    def last_layer(self, x):\n",
    "        return x\n",
    "    def preprocess_y(self, y):\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        return y\n",
    "    \n",
    "\n",
    "    def params_to_weights(self, params):\n",
    "        return [params[self.params_upto_l[l]:self.params_upto_l[l+1]].reshape(self.w[l].shape) for l in range(len(self.w))]\n",
    "    \n",
    "    def weights_to_params(self, weights):\n",
    "        return np.concatenate([w.flatten() for w in weights])\n",
    "    \n",
    "\n",
    "    def initialize(self, X, y, scale_weights=1):\n",
    "        y = self.preprocess_y(y)\n",
    "        n = X.shape[0]\n",
    "\n",
    "        # velikosti plasti\n",
    "        sizes = [X.shape[1]] + self.units + [y.shape[1]]\n",
    "        self.sizes = sizes\n",
    "\n",
    "        # incializira utezi in ustvari matrike gradientov\n",
    "        self.w = [np.random.normal(0, scale_weights / np.sqrt(sizes[i]), (sizes[i+1], sizes[i]+1)) for i in range(len(sizes)-1)]\n",
    "        self.dw = [np.zeros_like(w) for w in self.w]\n",
    "\n",
    "        # za pretvorbo vektorja parametrov v matrike\n",
    "        self.params_upto_l = [0] + [np.sum([np.prod(w.shape) for w in self.w[:l]]) for l in range(1, len(self.w)+1)]\n",
    "\n",
    "        # ustvari matrike aktivacij\n",
    "        self.a = [np.ones((sizes[l]+1, n)) for l in range(len(sizes))]\n",
    "        self.a[-1] = np.ones((sizes[-1], n))\n",
    "        self.delta = [np.zeros_like(a) for a in self.a]\n",
    "\n",
    "        self.n_weights = np.sum([np.prod(w[:, :-1].shape) for w in self.w])\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X, y = self.initialize(X, y)\n",
    "\n",
    "        params = self.weights_to_params(self.w)\n",
    "        params_opt, _, _ = fmin_l_bfgs_b(lambda ps, X, y: self.cost_and_grad(X, y, self.params_to_weights(ps)), params, args=(X.T, y.T))\n",
    "        # params_opt = params\n",
    "        self.w = self.params_to_weights(params_opt)\n",
    "\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def forward_pass(self, X, ws):\n",
    "        a = self.a\n",
    "        a[0][:-1, :] = X\n",
    "        for l in range(1, len(self.w)):\n",
    "            a[l][:-1, :] = self.sigmoid(ws[l-1] @ a[l-1])\n",
    "\n",
    "        l = len(self.w)\n",
    "        a[l] = self.last_layer(ws[l-1] @ a[l-1])\n",
    "        return a\n",
    "    \n",
    "\n",
    "    def cost_function(self, preds, y):\n",
    "        return 0.5 * np.sum((preds - y)**2) / y.shape[1]\n",
    "\n",
    "    def cost(self, y, aa, ws):\n",
    "        # izvzamemo zadnji stolpec utezi, ki predstavlja bias\n",
    "        return self.cost_function(aa[-1], y) + self.lambda_ / (2 * self.n_weights) * np.sum([np.sum(w[:, :-1]**2) for w in ws])\n",
    "    \n",
    "\n",
    "    \n",
    "    def last_layer_derivative(self, a, y):\n",
    "        return a - y\n",
    "\n",
    "\n",
    "    def cost_and_grad(self, X, y, ws=None):\n",
    "        if ws is None:\n",
    "            ws = self.w\n",
    "        dw = self.dw\n",
    "\n",
    "        n = X.shape[1]\n",
    "\n",
    "        # forward pass\n",
    "        aa = self.forward_pass(X, ws)\n",
    "        cost = self.cost(y, aa, ws)\n",
    "        # zadnji layer\n",
    "        delta = self.last_layer_derivative(aa[-1], y)\n",
    "        # backward pass\n",
    "        for l in range(len(ws)-1, -1, -1):\n",
    "            dw[l] = delta @ aa[l].T / n\n",
    "            dw[l][:, :-1] += self.lambda_ * ws[l][:, :-1] / self.n_weights\n",
    "            delta = (ws[l][:, :-1].T @ delta) * self.sigmoid_prime(aa[l][:-1, :])\n",
    "\n",
    "        return cost, self.weights_to_params(dw)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        # ustvari matrike aktivacij\n",
    "        n = X.shape[0]\n",
    "        self.a = [np.ones((self.sizes[l]+1, n)) for l in range(len(self.sizes))]\n",
    "        self.a[-1] = np.ones((self.sizes[-1], n))\n",
    "\n",
    "        preds = self.forward_pass(X.T, self.w)[-1]\n",
    "        if preds.shape[0] == 1:\n",
    "            return preds.flatten()\n",
    "        return preds.T\n",
    "\n",
    "    def weights(self):\n",
    "        return [w.T for w in self.w]\n",
    "    \n",
    "    def cost_at_params(self, params, X, y):\n",
    "        X = X.T\n",
    "        y = self.preprocess_y(y).T\n",
    "        ws = self.params_to_weights(params)\n",
    "        aa = self.forward_pass(X, ws)\n",
    "        return self.cost(y, aa, ws)\n",
    "\n",
    "\n",
    "class ANNClassification(ANNRegression):\n",
    "    def last_layer(self, x):\n",
    "        # softmax\n",
    "        x = x - np.mean(x, axis=0, keepdims=True)\n",
    "        u = np.exp(x)\n",
    "        return u / np.sum(u, axis=0, keepdims=True)\n",
    "\n",
    "    def preprocess_y(self, y):\n",
    "        y = super().preprocess_y(y)\n",
    "        # one hot encoding for final layer\n",
    "        y = (y == np.unique(y)).astype(int)\n",
    "        return y\n",
    "\n",
    "    def cost_function(self, preds, y):\n",
    "        # cross entropy\n",
    "        return -np.sum(y * np.log(preds)) / y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0],\n",
    "                    [0, 1],\n",
    "                    [1, 0],\n",
    "                    [1, 1]])\n",
    "y = np.array([0, 1, 2, 3])\n",
    "hard_y = np.array([0, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = hard_y\n",
    "# y = np.array([0, 1, 1, 1])\n",
    "fitter = ANNClassification(units=[3], lambda_=0.001)\n",
    "m = fitter.fit(X, y)\n",
    "# fitter.initialize(X, y)\n",
    "# m = fitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 1.1587204138\n",
      "Mean relative difference between numerical and analytical gradient:  7.2e-09\n",
      "Max relative difference between numerical and analytical gradient:  5.89e-08\n",
      "Stanard deviation of relative difference between numerical and analytical gradient:  1.73e-08\n",
      "Cost: 0.0154612865\n",
      "Mean relative difference between numerical and analytical gradient:  1.06812e-05\n",
      "Max relative difference between numerical and analytical gradient:  7.24198e-05\n",
      "Stanard deviation of relative difference between numerical and analytical gradient:  1.66335e-05\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "fitter = ANNClassification(units=[3], lambda_=0.001)\n",
    "fitter.initialize(X, hard_y)\n",
    "compare_numerical_gradient(fitter, X, hard_y)\n",
    "m = fitter.fit(X, hard_y)\n",
    "compare_numerical_gradient(m, X, hard_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.240934776\n",
      "Mean relative difference between numerical and analytical gradient:  4e-10\n",
      "Max relative difference between numerical and analytical gradient:  1.2e-09\n",
      "Stanard deviation of relative difference between numerical and analytical gradient:  3e-10\n",
      "Cost: 0.0020499033\n",
      "Mean relative difference between numerical and analytical gradient:  6.657e-07\n",
      "Max relative difference between numerical and analytical gradient:  8.0686e-06\n",
      "Stanard deviation of relative difference between numerical and analytical gradient:  2.1375e-06\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "fitter = ANNRegression(units=[3], lambda_=0.001)\n",
    "fitter.initialize(X, y)\n",
    "compare_numerical_gradient(fitter, X, y)\n",
    "m = fitter.fit(X, y)\n",
    "compare_numerical_gradient(m, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2609,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = m.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2610,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99741, 0.00259],\n",
       "       [0.003  , 0.997  ],\n",
       "       [0.00217, 0.99783],\n",
       "       [0.99741, 0.00259]])"
      ]
     },
     "execution_count": 2610,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2611,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002587959758798886"
      ]
     },
     "execution_count": 2611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy = m.preprocess_y(y)\n",
    "-np.sum(yy * np.log(pred)) / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2638,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost, grad = m.cost_and_grad(X.T, m.preprocess_y(y).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_numerical_gradient(model, X, y, eps=1e-6, precision=10):\n",
    "    cost, grad = model.cost_and_grad(X.T, model.preprocess_y(y).T)\n",
    "    params = model.weights_to_params(model.w)\n",
    "    errors = np.zeros_like(params)\n",
    "    estimates = np.zeros_like(params)\n",
    "    for i in range(len(params)):\n",
    "        params[i] += eps\n",
    "        c1 = model.cost_at_params(params, X, y)\n",
    "        params[i] -= 2*eps\n",
    "        c2 = model.cost_at_params(params, X, y)\n",
    "        params[i] += eps\n",
    "        estimate = (c1 - c2) / (2*eps)\n",
    "        estimates[i] = estimate\n",
    "        errors[i] = 2 * np.abs(estimate - grad[i]) / np.abs(estimate + grad[i])\n",
    "\n",
    "    print(\"Cost:\", cost.round(10))\n",
    "    print(\"Mean relative difference between numerical and analytical gradient: \", np.mean(errors).round(precision))\n",
    "    print(\"Max relative difference between numerical and analytical gradient: \", np.max(errors).round(precision))\n",
    "    print(\"Stanard deviation of relative difference between numerical and analytical gradient: \", np.std(errors).round(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.0154612865\n",
      "Mean relative difference between numerical and analytical gradient:  1.06812e-05\n",
      "Max relative difference between numerical and analytical gradient:  7.24198e-05\n",
      "Stanard deviation of relative difference between numerical and analytical gradient:  1.66335e-05\n"
     ]
    }
   ],
   "source": [
    "compare_numerical_gradient(m, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2639,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing2r = pd.read_csv('housing2r.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2640,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing2r = pd.read_csv('housing2r.csv')\n",
    "housing2r_y = housing2r.y.to_numpy()\n",
    "housing2r_X = housing2r.drop(columns=['y']).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2490,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timkm\\AppData\\Local\\Temp\\ipykernel_20284\\2989588649.py:10: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23.423880257476753"
      ]
     },
     "execution_count": 2490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitter = ANNRegression(units=[5], lambda_=0.1)\n",
    "m = fitter.fit(housing2r_X, housing2r_y)\n",
    "preds = m.predict(housing2r_X)\n",
    "\n",
    "def mse(y, preds):\n",
    "    return np.mean((y - preds)**2)\n",
    "\n",
    "mse(housing2r_y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.01611489103591"
      ]
     },
     "execution_count": 2422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import linear_regression as lr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(housing2r_X, housing2r_y)\n",
    "preds = lr.predict(housing2r_X)\n",
    "\n",
    "mse(housing2r_y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2643,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timkm\\AppData\\Local\\Temp\\ipykernel_20284\\3331413488.py:10: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timkm\\AppData\\Local\\Temp\\ipykernel_20284\\3331413488.py:10: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "C:\\Users\\timkm\\AppData\\Local\\Temp\\ipykernel_20284\\3331413488.py:10: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "C:\\Users\\timkm\\AppData\\Local\\Temp\\ipykernel_20284\\3331413488.py:10: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "y = housing2r_y\n",
    "x = housing2r_X\n",
    "k = 4\n",
    "\n",
    "n_poskusov = 3\n",
    "\n",
    "np.random.seed(42)\n",
    "idx = np.random.permutation(len(y))\n",
    "x, y = x[idx], y[idx]\n",
    "id_splits = np.array_split(idx, k)\n",
    "x_folds, y_folds = np.array_split(x, k), np.array_split(y, k)\n",
    "losses = [np.zeros(len(y)) for i in range(n_poskusov)]\n",
    "preds = [np.zeros(len(y)) for i in range(n_poskusov)]\n",
    "for i in range(k):\n",
    "    x_tr = np.concatenate([x_folds[j] for j in range(k) if j != i])\n",
    "    y_tr = np.concatenate([y_folds[j] for j in range(k) if j != i])\n",
    "    x_test = x_folds[i]\n",
    "    y_test = y_folds[i]\n",
    "\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(x_tr, y_tr)\n",
    "    p = lr.predict(x_test)\n",
    "\n",
    "    losses[0][id_splits[i]] = mse(y_test, p)\n",
    "    preds[0][id_splits[i]] = p\n",
    "\n",
    "    fitter = ANNRegression(units=[], lambda_=0.0)\n",
    "    m = fitter.fit(x_tr, y_tr)\n",
    "    p = m.predict(x_test)\n",
    "    losses[1][id_splits[i]] = mse(y_test, p)\n",
    "    preds[1][id_splits[i]] = p\n",
    "\n",
    "    fitter = ANNRegression(units=[5], lambda_=0.1)\n",
    "    m = fitter.fit(x_tr, y_tr)\n",
    "    p = m.predict(x_test)\n",
    "    losses[2][id_splits[i]] = mse(y_test, p)\n",
    "    preds[2][id_splits[i]] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2645,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LogisticRegression\n",
      "Loss 0: 42.03348383433988 +/- 1.457389798447459\n",
      "\n",
      "ANN no hidden layers\n",
      "Loss 1: 42.04763266732574 +/- 1.4571986133446344\n",
      "\n",
      "ANN hidden layer\n",
      "Loss 2: 29.1414232103974 +/- 0.827573393093739\n"
     ]
    }
   ],
   "source": [
    "imena = [\"LogisticRegression\", \"ANN no hidden layers\", \"ANN hidden layer\"]\n",
    "for i in range(n_poskusov):\n",
    "    print(\"\")\n",
    "    print(imena[i])\n",
    "    print(f\"Loss {i}: {losses[i].mean()} +/- {losses[i].std() / np.sqrt(len(losses[i]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing3 = pd.read_csv('housing3.csv')\n",
    "housing3_y = housing3.Class\n",
    "# encode categories as integers\n",
    "housing3.Class = pd.Categorical(housing3.Class)\n",
    "housing3.Class = housing3.Class.cat.codes\n",
    "housing3_y = housing3.Class.to_numpy()\n",
    "\n",
    "housing3_X = housing3.drop(columns=['Class']).to_numpy()\n",
    "# housing3_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2678180189024065"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from oldmodels import MultinomialLogReg\n",
    "\n",
    "logreg = MultinomialLogReg()\n",
    "model = logreg.build(housing3_X, housing3_y)\n",
    "preds = model.predict(housing3_X)\n",
    "\n",
    "def log_loss(y, p):\n",
    "    return -np.log(p[np.arange(len(y)), y])\n",
    "\n",
    "l1 = log_loss(housing3_y, preds)\n",
    "l1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2649,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.886"
      ]
     },
     "execution_count": 2649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy\n",
    "np.mean(housing3_y == np.argmax(preds, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2650,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timkm\\AppData\\Local\\Temp\\ipykernel_20284\\3331413488.py:10: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2298319324030689"
      ]
     },
     "execution_count": 2650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitter = ANNClassification(units=[3], lambda_=0.1)\n",
    "m = fitter.fit(housing3_X, housing3_y)\n",
    "preds = m.predict(housing3_X)\n",
    "\n",
    "l2 = log_loss(housing3_y, preds)\n",
    "l2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2651,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.898"
      ]
     },
     "execution_count": 2651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(housing3_y == np.argmax(preds, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2660,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timkm\\AppData\\Local\\Temp\\ipykernel_20284\\834874647.py:10: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "y = housing3_y\n",
    "x = housing3_X\n",
    "k = 4\n",
    "\n",
    "n_poskusov = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "idx = np.random.permutation(len(y))\n",
    "x, y = x[idx], y[idx]\n",
    "id_splits = np.array_split(idx, k)\n",
    "x_folds, y_folds = np.array_split(x, k), np.array_split(y, k)\n",
    "losses = [np.zeros(len(y)) for i in range(n_poskusov)]\n",
    "accuracies = [np.zeros(len(y)) for i in range(n_poskusov)]\n",
    "preds = [np.zeros((len(y), 2)) for i in range(n_poskusov)]\n",
    "for i in range(k):\n",
    "    x_tr = np.concatenate([x_folds[j] for j in range(k) if j != i])\n",
    "    y_tr = np.concatenate([y_folds[j] for j in range(k) if j != i])\n",
    "    x_test = x_folds[i]\n",
    "    y_test = y_folds[i]\n",
    "\n",
    "    model = logreg.build(x_tr, y_tr)\n",
    "    p = model.predict(x_test)\n",
    "    losses[0][id_splits[i]] = log_loss(y_test, p)\n",
    "    accuracies[0][id_splits[i]] = (y_test == np.argmax(p, axis=1))\n",
    "    preds[0][id_splits[i], :] = p\n",
    "\n",
    "    model = ANNClassification(units=[3], lambda_=0.5)\n",
    "    model.fit(x_tr, y_tr)\n",
    "    p = model.predict(x_test)\n",
    "    losses[1][id_splits[i]] = log_loss(y_test, p)\n",
    "    accuracies[1][id_splits[i]] = (y_test == np.argmax(p, axis=1))\n",
    "    preds[1][id_splits[i], :] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2661,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MultinomialRegression\n",
      "Loss 0: 0.32145574599727444 +/- 0.032664724150925965\n",
      "Accuracy 0: 0.872 +/- 0.0149409504383088\n",
      "\n",
      "ANN 1 hidden layer\n",
      "Loss 1: 0.2968973632275739 +/- 0.022186539798198898\n",
      "Accuracy 1: 0.864 +/- 0.01532997064576446\n"
     ]
    }
   ],
   "source": [
    "imena = [\"MultinomialRegression\", \"ANN 1 hidden layer\"]\n",
    "for i in range(n_poskusov):\n",
    "    print(\"\")\n",
    "    print(imena[i])\n",
    "    print(f\"Loss {i}: {losses[i].mean()} +/- {losses[i].std() / np.sqrt(len(losses[i]))}\")\n",
    "    print(f\"Accuracy {i}: {accuracies[i].mean()} +/- {accuracies[i].std() / np.sqrt(len(accuracies[i]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 93)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "train_X = df_train.drop(columns=['id', 'target']).to_numpy(dtype=np.float32)\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.target = df_train.target.map(lambda s: int(s[-1]) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = df_train.target.to_numpy()\n",
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler = scaler.fit(train_X)\n",
    "# train_X = scaler.transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(y, p):\n",
    "    return -np.log(p[np.arange(len(y)), y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, multi_class='multinomial')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import multinomial logistic regression from sklearn\n",
    "\n",
    "\n",
    "\n",
    "logreg = LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "logreg.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7673\n",
      "0.622205378313374\n",
      "0.6222053783133737\n"
     ]
    }
   ],
   "source": [
    "preds = logreg.predict(train_X)\n",
    "print(np.mean(train_y == preds))\n",
    "\n",
    "from sklearn.metrics import log_loss as sk_log_loss\n",
    "\n",
    "preds = logreg.predict_proba(train_X)\n",
    "print(sk_log_loss(train_y, preds))\n",
    "print(log_loss(train_y, preds).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn\n",
      "0.6422127909468668\n",
      "my\n",
      "0.5606179765470954\n",
      "sklearn\n",
      "0.6400726031788525\n",
      "my\n",
      "0.5505507002489533\n",
      "sklearn\n",
      "0.6420233480454398\n",
      "my\n",
      "0.5576981705560102\n",
      "sklearn\n",
      "0.6506914725278496\n",
      "my\n",
      "0.568917115199162\n"
     ]
    }
   ],
   "source": [
    "y = train_y\n",
    "x = train_X\n",
    "k = 4\n",
    "n_poskusov = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "idx = np.random.permutation(len(y))\n",
    "x, y = x[idx], y[idx]\n",
    "\n",
    "\n",
    "id_splits = np.array_split(idx, k)\n",
    "x_folds, y_folds = np.array_split(x, k), np.array_split(y, k)\n",
    "\n",
    "losses = [np.zeros(len(y)) for i in range(n_poskusov)]\n",
    "accuracies = [np.zeros(len(y)) for i in range(n_poskusov)]\n",
    "preds = [np.zeros((len(y), 9)) for i in range(n_poskusov)]\n",
    "\n",
    "models = []\n",
    "\n",
    "for i in range(k):\n",
    "    x_tr = np.concatenate([x_folds[j] for j in range(k) if j != i])\n",
    "    y_tr = np.concatenate([y_folds[j] for j in range(k) if j != i])\n",
    "    x_test = x_folds[i]\n",
    "    y_test = y_folds[i]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler = scaler.fit(x_tr)\n",
    "    x_tr = scaler.transform(x_tr)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    print(\"sklearn\")\n",
    "    logreg = LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "    logreg.fit(x_tr, y_tr)\n",
    "    p = logreg.predict_proba(x_test)\n",
    "    loss = log_loss(y_test, p)\n",
    "    print(loss.mean())\n",
    "    losses[0][id_splits[i]] = log_loss(y_test, p)\n",
    "    accuracies[0][id_splits[i]] = (y_test == np.argmax(p, axis=1))\n",
    "    preds[0][id_splits[i], :] = p\n",
    "\n",
    "    print(\"my\")\n",
    "    model = ANNClassification(units=[20], lambda_=0.3)\n",
    "    model.fit(x_tr, y_tr)\n",
    "    p = model.predict(x_test)\n",
    "    loss = log_loss(y_test, p)\n",
    "    print(loss.mean())\n",
    "    losses[1][id_splits[i]] = log_loss(y_test, p)\n",
    "    accuracies[1][id_splits[i]] = (y_test == np.argmax(p, axis=1))\n",
    "    preds[1][id_splits[i], :] = p\n",
    "\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MultinomialRegression\n",
      "Loss 0: 0.6437500536747522 +/- 0.004714560560621403\n",
      "Accuracy 0: 0.76204 +/- 0.001904389867647904\n",
      "\n",
      "ANN\n",
      "Loss 1: 0.5594459906378052 +/- 0.0040375631979530785\n",
      "Accuracy 1: 0.78574 +/- 0.001834953145995832\n"
     ]
    }
   ],
   "source": [
    "imena = [\"MultinomialRegression\", \"ANN\"]\n",
    "for i in range(n_poskusov):\n",
    "    print(\"\")\n",
    "    print(imena[i])\n",
    "    print(f\"Loss {i}: {losses[i].mean()} +/- {losses[i].std() / np.sqrt(len(losses[i]))}\")\n",
    "    print(f\"Accuracy {i}: {accuracies[i].mean()} +/- {accuracies[i].std() / np.sqrt(len(accuracies[i]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = models[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11878, 93)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(train_X)\n",
    "train_X = scaler.transform(train_X)\n",
    "\n",
    "test_X = df_test.drop(columns=['id']).to_numpy(dtype=np.float32)\n",
    "test_X = scaler.transform(test_X)\n",
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds1 = preds\n",
    "np.random.seed(42)\n",
    "model = ANNClassification(units=[20], lambda_=0.3)\n",
    "model.fit(train_X, train_y)\n",
    "preds = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.81048030e-04, 2.99572513e-01, 4.04495833e-01, ...,\n",
       "        1.82667558e-02, 7.62892304e-05, 1.38864712e-04],\n",
       "       [7.69196775e-04, 2.20415483e-05, 1.24113760e-05, ...,\n",
       "        1.19734770e-04, 9.98703288e-01, 2.01958193e-04],\n",
       "       [2.58173076e-04, 6.75001016e-02, 4.94813620e-04, ...,\n",
       "        2.87216676e-03, 2.39109926e-03, 1.64181154e-03],\n",
       "       ...,\n",
       "       [1.93783545e-05, 1.16840228e-03, 2.63070622e-04, ...,\n",
       "        1.05877926e-05, 1.20521221e-04, 5.02678777e-06],\n",
       "       [1.96420836e-02, 2.76328326e-04, 1.27300805e-04, ...,\n",
       "        1.03470531e-03, 7.23753978e-04, 5.65596751e-01],\n",
       "       [3.06228163e-06, 7.48346767e-01, 2.33591552e-01, ...,\n",
       "        1.46614201e-04, 5.43289141e-06, 1.17788065e-05]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = m.predict(test_X)\n",
    "\n",
    "# save using pickle\n",
    "import pickle\n",
    "\n",
    "with open('preds.pkl', 'wb') as f:\n",
    "    pickle.dump(preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.arange(1, preds.shape[0] + 1)\n",
    "\n",
    "df = pd.DataFrame(preds, columns=['Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6', 'Class_7', 'Class_8', 'Class_9'])\n",
    "df['id'] = ids\n",
    "df = df[['id', 'Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6', 'Class_7', 'Class_8', 'Class_9']]\n",
    "\n",
    "df.to_csv('final.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 353.62744533, 3128.81515638, 1526.13458757,  509.15399007,\n",
       "        535.26851987, 2716.22034738,  529.1706147 , 1659.44732256,\n",
       "        920.16201615])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddd = pd.read_csv('final.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddd = ddd.drop(columns=['id']).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02977163, 0.26341262, 0.12848414, 0.0428653 , 0.04506386,\n",
       "       0.22867657, 0.04455048, 0.13970764, 0.07746776])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddd.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0761679969514436e-09"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp = model.predict(train_X)\n",
    "\n",
    "np.min(np.min(ddd, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30281876, 0.17555113, 0.22487851, ..., 0.31122848, 0.28821334,\n",
       "       0.24330885])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(pp, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
