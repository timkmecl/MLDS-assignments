### Lab assignments for **Machine Learning for Data Science** course, UL FRI 2023/24
- *Lecturer:* Tomaž Hočevar
- *Assistant:* Marko Toplak, Bernarda Petek

*Some helper functions or function outlines (to use as a starting point) were provided to us along with the data and instructions by the staff.*

## Assignments:

1. **Trees and Forests**

    Implement classification trees and random forests, support numeric input variables and a binary target variable, and compute standard errors of misclassification rates.

	[solution](<01>),
	[report](<01/HW1.pdf>)

2. **Evaluation of Predictive Models, Loss estimation**

    Estimate and evaluate loss estimation methods for binary classification models using various techniques, including holdout and cross-validation, and analyze the implications on model performance and risk estimation.


	[solution](<02>),
	[report](<02/HW2.pdf>)

3. **Generalized Linear Models, Logistic Regression**

    Implement multinomial and ordinal logistic regression models using maximum likelihood estimation, apply the models to basketball shot data to provide insights and interpret coefficients, and create a data generating process to empirically demonstrate when ordinal logistic regression outperforms multinomial logistic regression.

	[solution](<03>),
	[report](<03/HW3.pdf>)

4. **Neural Networks**

    Implement multi-layer fully-connected ANNs for both classification and regression using backpropagation, verify gradient and cost compatibility, and apply ANNs to housing2r, housing3, and train.csv.gzdatasets.

	[solution](<04>),
	[report](<04/HW4.pdf>)

5. **SVM, Kernels**

    Implement kernelized ridge regression and Support Vector Regression (SVR) with polynomial and RBF kernels, apply them to both 1-dimensional sine and housing2r datasets, tune parameters, and compare the results.

	[solution](<05>),
	[report](<05/HW5.pdf>)

6. **Bayesian Inference, Bayesian Logistic Regression**

    Use Bayesian Logistic regression with MCMC to infer the relationship between shot success, angle, and distance from the basket, state your opinion about the beta coefficient for distance, visualize the posterior samples with a scatterplot and contours, compare results with a random subset, formulate probabilistic questions about angle and distance, and implement Laplace approximation for comparison.

	[solution](<06>),
	[report](<06/HW6.pdf>)

7. **Unsupervised Learning, PCA, t-SNE**

    Given the keywords for more than 10,000 articles from the page rtvslo.si, use PCA and t-SNE to reconstruct categories and explain the visualizations.

	[solution](<07>),
	[report](<07/HW7.pdf>)